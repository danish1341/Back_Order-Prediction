# -*- coding: utf-8 -*-
"""Back_Order _Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O4wBgWGPZZhISOpyA8fr_xMLTiKnWLNc
"""

import csv
import numpy as np
import pandas as pd
import seaborn as sns
import os
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier

df= pd.read_csv('/content/Kaggle_Training_Dataset_v2.csv')

df.shape # there are lot of rows with data (Can be duplicate values and NA values might present)

df.head()

df.info() # Gives an idea of the No of categorical data and no of No of numerical columns

df.describe()

df.nunique()

df.isnull().sum()

df=df.drop(['sku'],axis=1)

df['went_on_backorder'].value_counts() # shows the data is imbalance

sns.countplot(x='went_on_backorder', data=df)
plt.show()

df1= df.dropna(axis=0) # dropping rows with NA values

df.shape ,df1.shape #  as we can see here there are relatively less values of na than actual data frame

df2= df1.drop_duplicates() # dropping duplicates
df2.shape

df1.info()

"""Going Through Categorical and Numerical Data"""

numerical_cols= df1.select_dtypes(include=['number']).columns
categorical_cols= df1.select_dtypes(include=['object']).columns

categorical_cols= ['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk',
       'stop_auto_buy', 'rev_stop'] # Dealing an output Categorical Column with Label Encoder (Excluded)

len(numerical_cols),len(categorical_cols)

"""Splitting the Dataset"""

X=df1.drop('went_on_backorder', axis=1)
y=df1["went_on_backorder"]

# lael encoding for the transformation of output categorical data to Numerical data
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# importing the necessary libraries for handling categorical data
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.ensemble import RandomForestClassifier

"""Modelling(Random Forest Classifier)though Pipeline

"""

# by solving Categorical Columns to the numerical colms by one hot encoding
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(), categorical_cols)
    ])

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=4)

pipeline.fit(X_train, y_train)

"""Evaluation"""

from sklearn.model_selection import cross_val_score ,cross_val_predict
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score ,f1_score ,accuracy_score ,roc_auc_score

"""Training Evaluation Metrics"""

y_pred= pipeline.predict(X_train)

confusion_matrix(y_train, y_pred)

precision_score(y_train, y_pred)

recall_score(y_train, y_pred)

f1_score(y_train, y_pred)

accuracy_score(y_train , y_pred)

roc_auc_score(y_train , y_pred)

def roc_curve_acc(y_train, y_pred, method):
    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, y_pred)
    roc_auc = auc(false_positive_rate, true_positive_rate)
    plt.title('Receiver Operating Characteristic')
    plt.plot(false_positive_rate, true_positive_rate, color='cyan' ,label='%s AUC = %0.3f'% (method, roc_auc))
    plt.legend(loc = 'lower right')
    plt.plot([0,1],[0,1],'r--')
    plt.xlim([-0.1,1.2])
    plt.ylim([-0.1,1.2])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')

from sklearn.metrics import roc_curve ,auc
roc_curve_acc(y_train, y_pred, "RF")

